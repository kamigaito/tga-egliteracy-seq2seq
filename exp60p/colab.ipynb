{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq.ipynb.mod20200202",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsJRzbOgZ9UN",
        "colab_type": "text"
      },
      "source": [
        "### 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yERmi_cfOSzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ConfigArgParse\n",
        "!pip install git+https://github.com/pytorch/text.git@master#wheel=torchtext\n",
        "!apt-get install file autoconf libtool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSHf2qY9bQ09",
        "colab_type": "text"
      },
      "source": [
        "### パスなど環境変数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-fddmQwbPr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "pwd = os.getcwd()\n",
        "### This directory contains libraries and exexution files.\n",
        "APPS_DIR = pwd + '/apps'\n",
        "%env APPS_DIR=$APPS_DIR\n",
        "### This directory contains training, development and test set\n",
        "DATA_DIR = pwd + '/dataset'\n",
        "%env DATA_DIR=$DATA_DIR\n",
        "### This directory contains output directions.\n",
        "WORK_DIR = pwd + '/work'\n",
        "%env WORK_DIR=$WORK_DIR\n",
        "\n",
        "#japanese -> english\n",
        "%env LANG_PAIR=ja-en\n",
        "#english -> japanese\n",
        "#%env LANG_PAIR=en-ja\n",
        "\n",
        "%env DATASET=$DATA_DIR/kftt-data-1.0/data/tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8GWDwl-qtS4",
        "colab_type": "text"
      },
      "source": [
        "### 使用するライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-jvJ2a5aWDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "echo \"download and make apps...\"\n",
        "\n",
        "mkdir -p ${APPS_DIR}\n",
        "cd ${APPS_DIR}\n",
        "\n",
        "git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "git clone https://github.com/OpenNMT/OpenNMT-py.git\n",
        "git clone https://github.com/neubig/kytea.git\n",
        "cd kytea\n",
        "autoreconf -i\n",
        "./configure --prefix=${APPS_DIR}/kytea\n",
        "make\n",
        "make install\n",
        "\n",
        "echo \"finish apps preparation.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2YWdq3qykg",
        "colab_type": "text"
      },
      "source": [
        "### 使用するデータのダウンロードとデータ量調整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhZjOtao95lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p $WORK_DIR/input\n",
        "mkdir -p ${DATA_DIR}\n",
        "\n",
        "# Download data set\n",
        "wget -O ${DATA_DIR}/kftt-data-1.0.tar.gz http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
        "cd ${DATA_DIR}\n",
        "# Uncompress\n",
        "tar xvzf kftt-data-1.0.tar.gz\n",
        "\n",
        "paste \\\n",
        "  ${DATASET}/kyoto-train.cln.en \\\n",
        "  ${DATASET}/kyoto-train.cln.ja |\n",
        "  shuf > ${DATASET}/train.shuf\n",
        "\n",
        "#1p\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#20p\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "#head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#60p\n",
        "head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
        "head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
        "#100p\n",
        "#cut -f 1 ${DATASET}/train.shuf > ${DATASET}/train.en\n",
        "#cut -f 2 ${DATASET}/train.shuf > ${DATASET}/train.ja"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_pyMUHrW2tu",
        "colab_type": "text"
      },
      "source": [
        "###使用するデータのサンプル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itZl5fdyVj1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "train_name=train\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "sed -n 1P ${DATASET}/${train_name}.${src}\n",
        "sed -n 1P ${DATASET}/${train_name}.${trg}\n",
        "sed -n 2P ${DATASET}/${train_name}.${src}\n",
        "sed -n 2P ${DATASET}/${train_name}.${trg}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDICQ3RJW_S2",
        "colab_type": "text"
      },
      "source": [
        "###OpenNMT-pyライブラリで前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8KpDJetNPPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "suffix=\"en-ja ja-en\"\n",
        "train_name=train\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "    python preprocess.py \\\n",
        "    -train_src ${DATASET}/${train_name}.${src} \\\n",
        "    -train_tgt ${DATASET}/${train_name}.${trg} \\\n",
        "    -valid_src ${DATASET}/kyoto-dev.${src} \\\n",
        "    -valid_tgt ${DATASET}/kyoto-dev.${trg} \\\n",
        "    -save_data ${DATA_DIR}/dicts-${train_name}-${LANG_PAIR} \\\n",
        "    -src_words_min_frequency 5 \\\n",
        "    -tgt_words_min_frequency 5 \\\n",
        "    -src_seq_length 40 \\\n",
        "    -tgt_seq_length 40\n",
        "#train_src, train_tgt, valid_src, valid_tgt: text\n",
        "#save_data: bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v_K4UgrXNeP",
        "colab_type": "text"
      },
      "source": [
        "###前処理後の学習データ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q_WvnoV9BPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd {APPS_DIR}/OpenNMT-py\n",
        "import torch\n",
        "\n",
        "#学習データ。\n",
        "data = torch.load('/content/dataset/dicts-train-ja-en.train.0.pt')\n",
        "\n",
        "#vocab: データの種類とその処理方法の定義。\n",
        "vocab = torch.load('/content/dataset/dicts-train-ja-en.vocab.pt')\n",
        "\n",
        "print(\"学習データ数\", len(data))\n",
        "print(\"\")\n",
        "print(\"data[0]\")\n",
        "print(\"index\", data[0].indices)\n",
        "print(\"翻訳前\", data[0].src)\n",
        "print(\"翻訳後\", data[0].tgt)\n",
        "print(\"\")\n",
        "print(\"data[1]\")\n",
        "print(\"index\", data[1].indices)\n",
        "print(\"翻訳前\", data[1].src)\n",
        "print(\"翻訳後\", data[1].tgt)\n",
        "print(\"\")\n",
        "print(\"ニューラルネットに与えられるデータ。\")\n",
        "print(vocab['src'].fields[0][1].process((data[0].src[0],data[1].src[0])))\n",
        "print(\"最初のtensorは文に、二つ目のtensorは単語数に対応。単語が数値に変換されていることがわかる。\")\n",
        "print(\"処理の都合上、短い文は空白でパディングされているが、ニューラルネット内部ではマスクされて学習に影響が出ないようになっている、はず。\")\n",
        "print(\"また扱いやすさからonehot形式でなく整数になっているがニューラルネットの内部的にはonehot形式として扱われる。\")\n",
        "#今回はあまり関係ないが、data[0].srcが単語のリスト（文章）のリストになっているのは、\n",
        "#品詞(Part-of-Speech, POS)や固有表現(e.g. 東京 -> 地名, 1月1日 -> 日付, など)(Named Entity Recognition, NER)などのタグを同時に与える場合があるから。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkzauOrq3MP",
        "colab_type": "text"
      },
      "source": [
        "### ニューラルネットの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRlSf2dGKqOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "start_time=`date +%s`\n",
        "\n",
        "TIME_PATH=$(pwd)/train.time.log\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_NAME=train\n",
        "MODEL_DIR=${WORK_DIR}/models\n",
        "\n",
        "NUM_EPOCH=10\n",
        "\n",
        "mkdir -p ${MODEL_DIR}\n",
        "\n",
        "NUM_DATA=$(cat ${DATASET}/${TRAIN_NAME}.en | wc -l)\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "batch_size=256\n",
        "num_steps=$((${NUM_DATA}*${NUM_EPOCH}/${batch_size}))\n",
        "#モデルは左のファイルボタンから、\n",
        "#encoder: apps/OpenNMT-py/onmt/encoders/rnn_encoder.py (RNNEncoder)\n",
        "#decoder: apps/OpenNMT-py/onmt/encoders/decoder.py (StdRNNDecoder)\n",
        "#を参照。または授業資料を参照。\n",
        "python train.py \\\n",
        "    -data ${DATA_DIR}/dicts-${TRAIN_NAME}-${LANG_PAIR} \\\n",
        "    -save_model ${MODEL_DIR}/${LANG_PAIR} \\\n",
        "    -layers 2 \\\n",
        "    -rnn_size 500 \\\n",
        "    -word_vec_size 300 \\\n",
        "    -optim adam \\\n",
        "    -learning_rate 0.001 \\\n",
        "    -dropout 0.3 \\\n",
        "    -batch_size ${batch_size} \\\n",
        "    -report_every 1 \\\n",
        "    -save_checkpoint_steps ${num_steps} \\\n",
        "    -train_steps ${num_steps} \\\n",
        "    -gpu_rank 0 |& awk '(NR%30==0 || NR < 70){print}' #出力を少し抑制（重くなるので）\n",
        "cp ${MODEL_DIR}/${LANG_PAIR}_step_${num_steps}.pt ${MODEL_DIR}/${LANG_PAIR}_final.pt\n",
        "\n",
        "\n",
        "end_time=`date +%s`\n",
        "time=$((end_time - start_time))\n",
        "echo \"${time} (sec)\" >& $TIME_PATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capkxxMCmXVF",
        "colab_type": "text"
      },
      "source": [
        "###結果の評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qAuwS9VC1jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "\n",
        "mkdir -p ${OUT_DIR}\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "#学習したモデルを使ってsize1のビームサーチにより翻訳結果を取得。\n",
        "python translate.py \\\n",
        "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
        "    -src ${DATASET}/kyoto-test.${src} \\\n",
        "    -output ${OUT_DIR}/test.${trg} \\\n",
        "    -gpu 0 \\\n",
        "    -beam_size 1 \\\n",
        "    -batch_size 512 \\\n",
        "    -verbose"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmtWMzaDm0qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "\n",
        "cd ${APPS_DIR}/OpenNMT-py\n",
        "\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "for i in 1 2 3 4 5; do\n",
        "  echo [評価データの翻訳結果 $i]\n",
        "  sed -n ${i}P ${OUT_DIR}/test.${trg}\n",
        "  echo [評価データの正しい翻訳結果 $i]\n",
        "  sed -n ${i}P ${DATASET}/kyoto-test.${trg}\n",
        "done\n",
        "\n",
        "#bleuスコア算出。\n",
        "perl ${APPS_DIR}/mosesdecoder/scripts/generic/multi-bleu.perl \\\n",
        "    ${DATASET}/kyoto-test.${trg} \\\n",
        "    < ${OUT_DIR}/test.${trg} \\\n",
        "    1> ${OUT_DIR}/result_${LANG_PAIR}.bleu \\\n",
        "    2> /dev/null\n",
        "\n",
        "cat ${OUT_DIR}/result_${LANG_PAIR}.bleu | sed -r 's/(BLEU = [0-9]*\\.[0-9]*), .*/\\1/g' | tee ${SCORE_PATH}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzvx40A5Eaer",
        "colab_type": "text"
      },
      "source": [
        "###好きな文章を翻訳してみる\n",
        "#### （以下の`echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}`を書き換えて左のセル実行ボタンを押してみよう）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoExgq2kKqiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "MODELS=${WORK_DIR}/models\n",
        "OUT_DIR=${WORK_DIR}/outputs\n",
        "USRDIR=${WORK_DIR}/input\n",
        "SCORE_PATH=$(pwd)/score.txt\n",
        "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
        "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
        "\n",
        "echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}\n",
        "\n",
        "if [ ! -e ${USRDIR}/user.${src} ]\n",
        "then\n",
        "    echo \"${USRDIR}/user.${src} does not exist.\"\n",
        "    exit\n",
        "fi\n",
        "\n",
        "#tokenに分割。\n",
        "if [ ${src} = \"ja\" -a ${src} != \"en\" ]\n",
        "then\n",
        "    # Tokenize Japanese sentences\n",
        "    bash ${APPS_DIR}/kytea/src/bin/kytea \\\n",
        "    < ${USRDIR}/user.${src} |\\\n",
        "    sed 's/\\/[^ ]\\+//g' \\\n",
        "    > ${USRDIR}/user.tok.${src}\n",
        "elif [ ${src} = \"en\" -a ${src} != \"ja\" ]\n",
        "then\n",
        "    # Tokenize English sentences\n",
        "    perl ${APPS_DIR}/mosesdecoder/scripts/tokenizer/tokenizer.perl \\\n",
        "    -l en \\\n",
        "    < ${USRDIR}/user.${src} \\\n",
        "    > ${USRDIR}/user.tok.${src}\n",
        "else\n",
        "echo \"Language: ${src} is undefined.\"\n",
        "    continue\n",
        "fi\n",
        "\n",
        "#分割結果。\n",
        "cat ${USRDIR}/user.tok.${src}\n",
        "\n",
        "#あとは同様に翻訳する。\n",
        "python translate.py \\\n",
        "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
        "    -src ${USRDIR}/user.tok.${src} \\\n",
        "    -output ${OUT_DIR}/user.${trg} \\\n",
        "    -gpu 0 \\\n",
        "    -beam_size 1 \\\n",
        "    -batch_size 512 \\\n",
        "    -verbose"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu79Yj-9q10v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat train.time.log\n",
        "!cat score.txt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
